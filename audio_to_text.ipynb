{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model to be used and name of the file and video source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "# Define what model to use possible variations are\n",
    "# 'tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en' 'medium', 'large-v1', 'large-v2', 'large'\n",
    "# NOTE: Large models require more than 11GB VRAM\n",
    "model_name = \"medium.en\"\n",
    "audio_model = whisper.load_model(model_name)\n",
    "\n",
    "# Define url to download video and wanted filename\n",
    "\n",
    "name = 'SET YOUR OWN'\n",
    "\n",
    "url = 'SET YOUR OWN'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get video from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import m3u8_To_MP4\n",
    "# Download video\n",
    "if \".m3u8\" in url:\n",
    "    m3u8_To_MP4.multithread_download(url)\n",
    "    # Rename file\n",
    "    os.rename('m3u8_To_MP4.mp4', name+\".mp4\")\n",
    "elif \".mp4\" in url:\n",
    "    urllib.request.urlretrieve(url, name+\".mp4\")\n",
    "#elif \".ts\" in url:\n",
    "    #subprocess.run(['ffmpeg', '-i', url, name+\".mp4\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to mp3 and filter noise and silent parts out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: install ffmpeg with choco to windows\n",
    "mp4_file = name+\".mp4\"\n",
    "mp3_file = name+\".mp3\"\n",
    "txt_file = name+\".txt\"\n",
    "# ~5min for 1.5h file\n",
    "# Convert mp4 to mp3\n",
    "os.system('ffmpeg -i {} -vn -y {}'.format(mp4_file, mp3_file))\n",
    "# Remove noise\n",
    "os.system(\"ffmpeg -i {} -af afftdn=nr=9 -y {}\".format(mp3_file, \"tmp_\" + mp3_file))\n",
    "# Remove silent spots\n",
    "os.system(\"ffmpeg -i {} -af silenceremove=stop_periods=-1:stop_duration=3:stop_threshold=-50dB -y {}\".format(\"tmp_\" + mp3_file, mp3_file))\n",
    "# Delete temporary file\n",
    "os.remove(\"tmp_\" + mp3_file)\n",
    "os.remove(mp4_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = {\"en\": \"english\",\"zh\": \"chinese\",\"de\": \"german\",\"es\": \"spanish\",\"ru\": \"russian\",\"ko\": \"korean\",\"fr\": \"french\",\"ja\": \"japanese\",\"pt\": \"portuguese\",\"tr\": \"turkish\",\"pl\": \"polish\",\"ca\": \"catalan\",\"nl\": \"dutch\",\"ar\": \"arabic\",\"sv\": \"swedish\",\"it\": \"italian\",\"id\": \"indonesian\",\"hi\": \"hindi\",\"fi\": \"finnish\",\"vi\": \"vietnamese\",\"he\": \"hebrew\",\"uk\": \"ukrainian\",\"el\": \"greek\",\"ms\": \"malay\",\"cs\": \"czech\",\"ro\": \"romanian\",\"da\": \"danish\",\"hu\": \"hungarian\",\"ta\": \"tamil\",\"no\": \"norwegian\",\"th\": \"thai\",\"ur\": \"urdu\",\"hr\": \"croatian\",\"bg\": \"bulgarian\",\"lt\": \"lithuanian\",\"la\": \"latin\",\"mi\": \"maori\",\"ml\": \"malayalam\",\"cy\": \"welsh\",\"sk\": \"slovak\",\"te\": \"telugu\",\"fa\": \"persian\",\"lv\": \"latvian\",\"bn\": \"bengali\",\"sr\": \"serbian\",\"az\": \"azerbaijani\",\"sl\": \"slovenian\",\"kn\": \"kannada\",\"et\": \"estonian\",\"mk\": \"macedonian\",\"br\": \"breton\",\"eu\": \"basque\",\"is\": \"icelandic\",\"hy\": \"armenian\",\"ne\": \"nepali\",\"mn\": \"mongolian\",\"bs\": \"bosnian\",\"kk\": \"kazakh\",\"sq\": \"albanian\",\"sw\": \"swahili\",\"gl\": \"galician\",\"mr\": \"marathi\",\"pa\": \"punjabi\",\"si\": \"sinhala\",\"km\": \"khmer\",\"sn\": \"shona\",\"yo\": \"yoruba\",\"so\": \"somali\",\"af\": \"afrikaans\",\"oc\": \"occitan\",\"ka\": \"georgian\",\"be\": \"belarusian\",\"tg\": \"tajik\",\"sd\": \"sindhi\",\"gu\": \"gujarati\",\"am\": \"amharic\",\"yi\": \"yiddish\",\"lo\": \"lao\",\"uz\": \"uzbek\",\"fo\": \"faroese\",\"ht\": \"haitian creole\",\"ps\": \"pashto\",\"tk\": \"turkmen\",\"nn\": \"nynorsk\",\"mt\": \"maltese\",\"sa\": \"sanskrit\",\"lb\": \"luxembourgish\",\"my\": \"myanmar\",\"bo\": \"tibetan\",\"tl\": \"tagalog\",\"mg\": \"malagasy\",\"as\": \"assamese\",\"tt\": \"tatar\",\"haw\": \"hawaiian\",\"ln\": \"lingala\",\"ha\": \"hausa\",\"ba\": \"bashkir\",\"jw\": \"javanese\",\"su\": \"sundanese\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect language if english model not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \".en\" not in model_name:\n",
    "    # Detect language\n",
    "    audio = whisper.load_audio(mp3_file)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(audio_model.device)\n",
    "    _, probs = audio_model.detect_language(mel)\n",
    "    detected_language = LANGUAGES[max(probs, key=probs.get)]\n",
    "    print(f\"Detected language: {detected_language}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe audio takes about 9 min with gtx1070 to trancsribe 1.5h audio using medium.en\n",
    "if \".en\" not in model_name:\n",
    "    result = audio_model.transcribe(mp3_file, verbose= True, language=detected_language)\n",
    "else:\n",
    "    result = audio_model.transcribe(mp3_file, verbose= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split each sentence to own row and delete .mp3-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(mp3_file)\n",
    "\n",
    "with open(txt_file, 'w') as f:\n",
    "    # Write data to the file\n",
    "    f.write(result[\"text\"])\n",
    "    \n",
    "try:\n",
    "    # Open the file for reading\n",
    "    with open(txt_file, 'r') as file:\n",
    "        contents = file.read()\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create a empty to be saved later\n",
    "    contents = ''\n",
    "\n",
    "# Split text where sentences end\n",
    "contents = contents.replace('\\n', '')\n",
    "contents = contents.replace('?', '?\\n')\n",
    "contents = contents.replace('.', '.\\n')\n",
    "contents = contents.replace('!', '!\\n')\n",
    "# Only use this if some sentences are really long.\n",
    "#contents = contents.replace(',', ',\\n')\n",
    "\n",
    "# Open the file for writing and overwrite the existing contents\n",
    "with open(txt_file, 'w') as file:\n",
    "    file.write(contents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedd sentences and calulate cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentences = []\n",
    "with open(txt_file, 'r') as file:\n",
    "        for line in file:\n",
    "                # Remove too short lines\n",
    "                if len(line)>6:\n",
    "                        sentences.append(line)\n",
    "\n",
    "# Define model to be used to be used for text embedding and create similarity matrix\n",
    "text_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = text_model.encode(sentences)\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "# Plot the similarity heatmap\n",
    "\"\"\"sns.heatmap(similarities).set_title('Cosine similarities matrix {}'.format(name))\n",
    "plt.savefig('{}.png'.format(name), dpi=2000)\"\"\"\n",
    "\n",
    "# Get diagonal and the one next to it for sentence similarity checking\n",
    "diagonals = [similarities.diagonal(), similarities.diagonal(1)+similarities.diagonal(1)[-1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get diagonals and detect where paragraphs should end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate differences\n",
    "difference = [diagonals[0][i]-diagonals[1][i] for i in range(len(diagonals[0])-1)]\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "# Perform n-wide Gaussian smoothing\n",
    "n = 1\n",
    "difference_smoothed = gaussian_filter1d(difference, n)\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Detect peaks where sentence difference is largest\n",
    "peaks, _ = find_peaks(difference_smoothed, distance=5)\n",
    "\n",
    "\"\"\"\n",
    "# Plot peaks\n",
    "plt.clf()\n",
    "plt.plot(difference_smoothed)\n",
    "plt.plot(peaks, difference_smoothed[peaks], \"x\")\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse final text file using detected paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format text for last step and remove too short sentences.\n",
    "with open(txt_file, 'r') as file:\n",
    "            contents = [l.rstrip(\"\\n\").lstrip() for l in file.readlines() if len(l)>6]\n",
    "\n",
    "# Add paragraph breaks to detected points and write to file\n",
    "text = ''\n",
    "for index, sentence in enumerate(contents):\n",
    "        if index-1 in peaks:\n",
    "                text += \"\\n\\n\"\n",
    "                text += sentence\n",
    "        else:\n",
    "                text += \" \"+sentence\n",
    "\n",
    "with open(txt_file, 'w') as file:\n",
    "        file.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40118f5f031a3101398604df892af6a231730f3865c9218da6290b373807c8b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
